\chapter{Discussion} % (fold)
\label{cha:discussion}

\section{Diagonal Matrices} % (fold)
\label{sec:diagonal_matrices}
The computational complexity can be reduced by introducing diagonal matrices instead of full matrices. In the derivation of the unknown parameters in \eqref{eq:param_computed} there is a huge difference in using diagonal matrices compared to full matrices. However, these derivations are done off line and therefore, there is nothing to gain in this step by introducing diagonal matrices.

% consist of four matrix multiplications. The number of multiplications in a matrix multiplication of a $a\times b$ matrix and a $b\times c$ matrix is $O(abc)$.
% 
% In the full covariance matrices case it is first a multiplication of a $m(1+p)\times n$ matrix times a $n\times m(1+p)$ matrix yielding $O(n(mp)^2)$ multiplications. The next step is the inverse of the resulting $m(p+1)\times m(p+1)$ matrix which has $O((mp)^3)$ multiplications. The last two multiplications have respectively $O((mp)^2n)$ and $O(mp^2n)$. The total number of multiplications for full covariance matrix is
% \begin{equation}
% 	\begin{split}
% 		O_f(\cdot) = & n(mp)^2 + (mp)^3 + (mp)^2n + mp^2n \\
% 		= & (mp)^3 + 2n(mp)^2 + nmp^2
% 	\end{split}
% \end{equation}
% 
% If the matrices are constrained to be diagonal the number of calculations will be respectively $O(nm^2)$, $O(m^3)$, $O(nm^2)$ and $O(nm)$ but all multiplications are done $p$ times.
% \begin{equation}
% 	\begin{split}
% 		O_d(\cdot) = & p(nm^2 + m^3 + nm^2 + mn) \\
% 		= & pm^3 + 2pnm^2 + pnm
% 	\end{split}
% \end{equation}
% The ratio between the two options is
% \begin{equation}
% 	\begin{split}
% 		\frac{O_f(\cdot)}{O_d(\cdot)} = & \frac{(mp)^3 + 2n(mp)^2 + nmp^2}{pm^3 + 2pnm^2 + pnm} \\
% 		= & \frac{(pm)^2 + 2nmp + np}{m^2 + 2nm + n} \\
% 		= & \frac{p(pm^2 + 2nm + n)}{m^2 + 2nm + n}
% 	\end{split}
% \end{equation}

The transformation function is applied on line; hence, there could be a gain from the use of diagonal matrices. In the transformation function \eqref{eq:conversion_function} both methods have to calculate the conditional probability matrix $P$ \eqref{eq:P_matrix}. Besides the $P$ matrix, the derivation consists of a summation of two matrix multiplications and a vector times a constant, for each input vector, if the inversion of the covariance matrix $\mathbf{\Sigma}^{xx}$ is done off line. 

In the case of full matrices, the number of multiplications is
\begin{equation}
	O_f = nm(p^3 + p^2+p)
\end{equation}
For diagonal matrices, all elements in the summation are variables instead of matrices and vectors, but the summation has to be computed $p$ times
\begin{equation}
	O_d = nmp\times3
\end{equation}
The ratio of the complexities is
\begin{equation}
	\begin{split}
		\frac{O_f}{O_d} = & \frac{nm(p^3 + p^2+p)}{3nmp} \\
		= & \frac{p^2 + p+1}{3}
	\end{split}
\end{equation}

The total number of multiplications and the ratios for $p=16$, $n=600$ and $m\in \{16,32,64,128,256\}$ are shown in Table~\ref{tab:derivation_complexity}.
\begin{table}[ht]
	\begin{center}
		\topcaption{Number of Multiplications in the Transformation Function.}
		\begin{tabular}{rll}
			\toprule
			\multicolumn{1}{c}{$m$} & \multicolumn{1}{c}{$O_f(\cdot)$} & \multicolumn{1}{c}{$O_d(\cdot)$} \\
			\midrule
			16 & $4.19 \times 10^7$ & $4.61 \times 10^5$ \\
			32 & $8.39 \times 10^7$ & $9.22 \times 10^5$ \\
			64 & $1.68 \times 10^8$ & $1.84 \times 10^6$ \\
			128 & $3.35 \times 10^8$ & $3.69 \times 10^6$ \\
			256 & $6.71 \times 10^8$ & $7.37 \times 10^6$ \\
			\bottomrule			
		\end{tabular}		
	\end{center}
\label{tab:derivation_complexity}	
\end{table}

By introducing diagonal matrices, the number of calculations has been reduced by a factor of $(p^2+p+1)/3$, only dependent of the number of prediction coefficients. On the other hand, a diagonal matrix system needs 8 times more mixtures to achieve the same performance as a full matrix system, if the vectors are correlated, yielding a complexity decrease factor of $(p^2+p+1)/24$ \cite{stylianou98}. A proof that \eqref{eq:param_computed} is solvable by the use of diagonal matrices is shown in Appendix~\ref{cha:proof_of_matrix_multiplication}. 
% section Diagnoal Matrices (end)


\section{Number of Mixtures and Training Vectors} % (fold)
\label{sec:number_of_mixture}
A training set of 40 000 LSF vectors, derived from approximately 6 minutes of speech, was used in the training of the GMM. Each vector was a linear prediction of 10 ms with an analysis window of two pitch periods. If the amount of training vectors per mixture was small, \eg $<100$, the covariance in the mixture models also got small, and there was an underflow issue when determining the inverse of the covariance matrix. However, a lower bound for the covariance matrices was set to $10^{-5}$ to ensure convergence in the parameter estimation. The parameter estimation also needs a high number of training vectors per mixture, approximately 100, to achieve a reasonable result.

The number of mixtures can be chosen to achieve the desired quality of the transformation. Table~\ref{tab:derivation_complexity} shows that for diagonal matrices the complexity of the transformation function increases linearly with the number of mixtures, which motivates using a high number of mixtures.

For the test with vectors from the training set, Table~4.1 shows clearly the increase in performance by utilising more mixture components. With fewer mixtures the resulting frequency spectrum is more smoothed out as shown in Figure~\ref{fig:comparison}. Table~4.2 -- 4.4 show that increasing the training data improves the accuracy slightly when the test data is taken from the training set. Since the only thing done in the training is estimating the expectation of the target vectors and the cross-covariance between the source and the target, it is naturally that the best estimate comes from a training set only containing the test data. There was also an improvement by increasing the number of mixture components. Increasing the number of mixtures can be seen as an analogy of increasing the size of the codebook in a VQ system, the range of possible outcomes is increased. 

In a real system, the only interesting test is a test with data not included in the training. The impact of the number of mixtures for this kind of test is depicted in Figure~\ref{fig:frequency_plots_1} and the accuracy is measured in Table~4.2 -- 4.4. Stylianou \etal \cite{stylianou98} showed in their work that the performance should increase with the number of mixture components, however they did not test the effect of altering the amount of training data. The presented results show the opposite trend for $S_{test}\notin S_{training}$. The effect of increasing the number of mixture models degrades the accuracy, for test data outside the training set. This was not the case for test set included in the training data. The performance increased with an increasing amount of training data, which was anticipated. Furthermore, the results from the test of $S_{test}\in S_{training}$ were almost as bad as the test with $S_{test}\notin S_{training}$, when the size of the training set was large. This can be interpreted to assert that the training set is representative for the possible frequency spectra of the source speaker. However, the accuracy of the transform has a very large standard deviation. The test with 256 mixture components and $40\times 10^3$ training vectors, had a dynamic range of 0.0700 -- 4.8292, \ie some vectors were transformed almost perfect. The standard deviation decreased when the number of training vectors increased, which means that the transformation increased it's stability. 

The target envelopes in Figure~\ref{fig:frequency_plots_2} and \ref{fig:frequency_plots_3} are computed by dynamic time warping, Section~\ref{sec:dynamic_time_warping}, for testing purposes only. There is a large difference in the source and target envelopes in Figure~\ref{fig:freq_1} and \ref{fig:freq_4}, which means that the DTW did not find a good match of the segments. The relationships between the source and target envelopes are similar in the two figures, only they have swapped places. The source envelope in Figure~\ref{fig:freq_1} was transformed quite precisely, but the envelope in \ref{fig:freq_4} was not. There seems to be a problem of transforming distinctive formant frequencies below $4$ kHz. Figure~\ref{fig:frequency_plots_3} shows that the formant frequencies in the source envelope got smoothed out, which might indicate that the number of mixture components is too small. Whilst in Figure~\ref{fig:frequency_plots_2}, the source does not have very distinctive formants for low frequency and the transform is more accurate.

Compared to Stylianou's work \cite{stylianou98} the presented implementation does not work properly. Increasing the number of mixture models should increase the transformation accuracy. The inconsistent results presented above could be due to errors in the implementation. Most probably, there is a timing issue in the dynamic time warping. The DTW could have been tested and tuned more for the feature comparison in question, for instance the DTW could be enhanced by using phonetic labelling and using a phone comparison. Another issue could be the number of mixture components. Stylianou used up to 1024 mixture components with diagonal matrices which requires more computational power than were used in the presented test. Another issue could be the training data, which was chosen randomly, while Stylianou used hand picked data that was representative for all diphones in his test language. Stylianou did not use line spectral frequencies as signal representation but a Harmonic Noise Model with cepstral coefficients and the transformation was only applied to voiced sounds. The impact of the choice of signal representation was not tested.
% section Number of Mixture (end)

\section{Synthesis} % (fold)
\label{sec:synthesis}
The voices used in the test had very different characteristics in pitch, pace and frequency spectra. If the synthesis of the signal with the transformed filter coefficients is done in a straight forward approach the result of the synthesis has a very low signal to noise ratio. To cope with this problem the LPC analysis was centred on the pitch pulses in the signal. This introduces another problem --- detecting the pitch. The pitch can be detected by finding the highest value of the autocorrelation of the region of interest, discussed in detail in \cite[p. 324]{taletek}. In the presented test result the pitch pulsed of the training data was pre-labelled. A test of a perfect transformation, \ie synthesis with the target parameters, proved that pitch synchronous filtering improved the signal to noise ratio significantly. However, for the transformed filter parameters, the timing is not correct and the noise is not suppressed.

A full speech transformations system includes a source modification as well, Figure~\ref{fig:VC_full}. The source modifications can be done by PSOLA \cite[p.820]{taletek} to alter the pitch and time-scale of the speech signal.
\begin{figure}[htbp]
  \centering
   \begin{tabular}[h]{c}
\xymatrix{ 
\mathbf{x} \ar[rr] & \ar[d]\ar[r] &*+<5mm>[F-,]{1-A(z)} \ar[r]^{\mathbf{e}}&*+<5mm>[F-,]{\txt{SM}} \ar[r]^(0.35){\mathbf{\tilde{e}}} &*+<5mm>[F-,]{\frac{1}{1-\tilde{A}(z)}} \ar[r] & \mathbf{y}\\  % end line 1
  &*+<5mm>[F-,]{\txt{LPC}} \ar[rr]^<(0.25){A(z)} & \ar[u]&*+<5mm>[F-,]{\txt{FT}}\ar `[ru]^<(0.5){\tilde{A}(z)} [ur]&  &}
  \end{tabular}
  \caption{Full transformation system.}
  \label{fig:VC_full}
\end{figure}
A source modification with PSOLA would solve the timing issue and increase the sound quality. This feature was however not implemented in the presented system. To further improve the sound quality one could apply advance concatenation of segments. Instead of just adding one segment after another, one could interpolate or smoothen the concatenation \cite{chappell02}.

Even though there was a lot of noise in the output signal, the transformed spectrum was audible. Naturally, with the presented transformation accuracy it was not possible to identify the synthesised sound signal as the target speaker.

% section Synthesis (end)

% chapter Discussion (end)