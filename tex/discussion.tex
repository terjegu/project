\chapter{Discussion} % (fold)
\label{cha:discussion}

\section{Diagonal Matrices} % (fold)
\label{sec:diagonal_matrices}
The computational complexity can be reduced by introducing diagonal matrices instead of full matrices. In the derivation of the unknown parameters in \eqref{eq:param_computed} there is a huge difference in using diagonal matrices compared to full matrices, but these derivations are done off line and therefor there is nothing to gain in this step by introducing diagonal matrices.

% consist of four matrix multiplications. The number of multiplications in a matrix multiplication of a $a\times b$ matrix and a $b\times c$ matrix is $O(abc)$.
% 
% In the full covariance matrices case it is first a multiplication of a $m(1+p)\times n$ matrix times a $n\times m(1+p)$ matrix yielding $O(n(mp)^2)$ multiplications. The next step is the inverse of the resulting $m(p+1)\times m(p+1)$ matrix which has $O((mp)^3)$ multiplications. The last two multiplications have respectively $O((mp)^2n)$ and $O(mp^2n)$. The total number of multiplications for full covariance matrix is
% \begin{equation}
% 	\begin{split}
% 		O_f(\cdot) = & n(mp)^2 + (mp)^3 + (mp)^2n + mp^2n \\
% 		= & (mp)^3 + 2n(mp)^2 + nmp^2
% 	\end{split}
% \end{equation}
% 
% If the matrices are constrained to be diagonal the number of calculations will be respectively $O(nm^2)$, $O(m^3)$, $O(nm^2)$ and $O(nm)$ but all multiplications are done $p$ times.
% \begin{equation}
% 	\begin{split}
% 		O_d(\cdot) = & p(nm^2 + m^3 + nm^2 + mn) \\
% 		= & pm^3 + 2pnm^2 + pnm
% 	\end{split}
% \end{equation}
% The ratio between the two options is
% \begin{equation}
% 	\begin{split}
% 		\frac{O_f(\cdot)}{O_d(\cdot)} = & \frac{(mp)^3 + 2n(mp)^2 + nmp^2}{pm^3 + 2pnm^2 + pnm} \\
% 		= & \frac{(pm)^2 + 2nmp + np}{m^2 + 2nm + n} \\
% 		= & \frac{p(pm^2 + 2nm + n)}{m^2 + 2nm + n}
% 	\end{split}
% \end{equation}

The transformation function is applied on line, hence there could be a gain from the use of diagonal matrices. In the transformation function \eqref{eq:conversion_function} both methods have to calculate the conditional probability matrix $P(C\vert \mathbf{x})$. Besides the $P$ matrix the derivation consist two matrix multiplications if the inversion of the covariance matrix $\mathbf{\Sigma}^{xx}$ is done off line.

In the full matrix case the number of multiplications is
\begin{equation}
	O_f = nm(p^3 + p^2)
\end{equation}
and for diagonal matrices all elements in the summation is variables instead of matrices and vectors, but the summation has to be computed $p$ times
\begin{equation}
	O_d = nmp.
\end{equation}
The ratio of the complexities is
\begin{equation}
	\begin{split}
		\frac{O_f}{O_d} = & \frac{nm(p^3 + p^2)}{nmp} \\
		= & p^2 + p.
	\end{split}
\end{equation}

The total number of multiplications and the ratios for $p=16$, $n=600$ and $m\in \{16,32,64,128,256\}$ are shown in Table~\ref{tab:derivation_complexity}.
\begin{table}[ht]
	\begin{center}
		\topcaption{Number of Multiplications in the Transformation Function}
		\begin{tabular}{rll}
			\toprule
			\multicolumn{1}{c}{$m$} & \multicolumn{1}{c}{$O_f(\cdot)$} & \multicolumn{1}{c}{$O_d(\cdot)$} \\
			\midrule
			16 & $4.18 \times 10^7$ & $1.54 \times 10^5$ \\
			32 & $8.36 \times 10^7$ & $3.07 \times 10^5$ \\
			64 & $1.67 \times 10^8$ & $6.14 \times 10^5$ \\
			128 & $3.34 \times 10^8$ & $1.23 \times 10^6$ \\
			256 & $6.68 \times 10^8$ & $2.56 \times 10^6$ \\
			\bottomrule			
		\end{tabular}		
	\end{center}
\label{tab:derivation_complexity}	
\end{table}

By introducing diagonal matrices the number of calculations has been reduced by a factor of $p^2+p$ only dependent of the number of prediction coefficients. On the other hand, diagonal matrices needs 8 times more mixtures to achieve the same performance as a full matrix system if the vectors are correlated, yielding a complexity decrease factor of $(p^2+p)/8$ \cite{stylianou98}. A proof that \eqref{eq:param_computed} is solvable by the use of diagonal matrices is shown in Appendix~\ref{cha:proof_of_matrix_multiplication}. 
% section Diagnoal Matrices (end)


\section{Number of Mixtures and Training Data} % (fold)
\label{sec:number_of_mixture}
A speech database of 40 000 LSF vectors derived from 7 minutes of speech was used in the training of the GMM. Each vector was a linear prediction of \ca 10 ms. If the amount training vectors per mixture is small, \eg $<100$, the covariance in the mixture models will get small and there could be a underflow issue when determine the inverse of the covariance matrix. However, a lower bound for the covariance matrices was set to $10^{-5}$ in ensure convergence in the parameter estimation.

The number of mixtures can be chosen to achieve the desired quality of the transformation. Table~\ref{tab:derivation_complexity} shows that for diagonal matrices the complexity of the transformation function increases linearly with the number of mixtures which motivates using a high number of mixtures.

For the test with vectors from the training set, Table~4.1 shows clearly the increase in performance by utilising more mixture components. With less mixtures the resulting frequency spectrum is more smoothed out as shown in Figure~\ref{fig:comparison}. Table~4.2 -- 4.4 show that increasing the training data decreases the accuracy when the test data is taken from the training set. Since the only thing done in the training is estimating the expectation of the target vectors and the cross-covariance between the source and the target, it is naturally that the best estimate comes from a training set only containing the test data.

In a real system we are only interested in test data outside the training set. The impact of the number of mixtures for this kind of test is depicted in Figure~\ref{fig:frequency_plots_1} and the accuracy is measured in Table~4.2 -- 4.4. Stylianou \etal \cite{stylianou98} showed in their work that the performance should increase with the number of mixture components, however they did not test the effect of the size of the training data. The presented results are shows the opposite. The effect of increasing the number of mixture models decreases the accuracy for test data outside the training setm which seems inaccurate. While the performance increases when the amount of training data is increased, which was anticipated. Furthermore the results from the test where the test set was taken from the training set was almost as bad as the test with test vectors outside the training set, when the size of the training set was large. However, the accuracy of the transform has a very large standard deviation. For the test with 256 mixture components and $40\times 10^3$ training vectors had a distance range of 0.0700 -- 4.8292, \ie some vectors were transformed almost perfect. The standard deviation does decrease when the number of training vectors increases, which is intuitive. 

It seems to be little or no correlation between the number of training vectors and the transformation accuracy for $S_{test}\notin S_{training}$. Increasing the number of mixture components decreases the accuracy for $S_{test}\notin S_{training}$, while increasing the number of training vectors increases the performance.

Compared to Stylianou's work \cite{stylianou98} the presented implementation does not work properly. The non-consistent results presented above could be due to errors in the implementation. Most probably the system needs more training data and higher order of mixture components to achieve an acceptable quality. There could be a timing issue in the dynamic time warping. The DTW could for instance be enhanced by using phonetic labelling of the training data. Another issue could be the training data which were chosen randomly, while Stylianou used hand picked data which where representative for all diphones in his test language. Stylianou did not use line spectral frequencies as signal representation but a Harmonic Noise Model with cepstral coefficients and the transformation was only applied to voiced sounds. 
% section Number of Mixture (end)



\section{Synthesis} % (fold)
\label{sec:synthesis}
If the synthesis of the signal with the transformed filter coefficients is done in a straight forward approach the result of the synthesis has a very low signal to noise ratio. To cope with this problem the LPC analysis was centred around the pitch pulses in the signal. This introduces another problem --- detecting the pitch. The pitch can be detected by finding the highest value of the autocorrelation of the region of interest, discussed in detail in \cite[p. 324]{taletek}. In the presented test result the pitch pulsed of the training data were pre-labelled. A of a perfect transformation, \ie synthesis with the target parameters, proved that pitch synchronous filtering improved the signal to noise ratio significantly. However, for the transformed filter parameters the timing is not correct and noise is not suppressed.

A full speech transformations system includes a source modification as well, Figure~\ref{fig:VC_full}. The source modifications can be done by PSOLA \cite[p.820]{taletek} to alter the pitch and time-scale  of the speech signal.
\begin{figure}[htbp]
  \centering
   \begin{tabular}[h]{c}
\xymatrix{ 
\mathbf{x} \ar[rr] & \ar[d]\ar[r] &*+<5mm>[F-,]{1-A(z)} \ar[r]^{\mathbf{e}}&*+<5mm>[F-,]{\txt{SM}} \ar[r]^(0.35){\mathbf{\tilde{e}}} &*+<5mm>[F-,]{\frac{1}{1-\tilde{A}(z)}} \ar[r] & \mathbf{y}\\  % end line 1
  &*+<5mm>[F-,]{\txt{LPC}} \ar[rr]^<(0.25){A(z)} & \ar[u]&*+<5mm>[F-,]{\txt{FT}}\ar `[ru]^<(0.5){\tilde{A}(z)} [ur]&  &}
  \end{tabular}
  \caption{Full Transformation System}
  \label{fig:VC_full}
\end{figure}
A source modification with PSOLA would solve the timing issue and increase the sound quality. This feature was however not implemented in the presented system. To further improve the sound quality one could apply advance concatenation of segments. In stead of just adding one segment after another one could interpolate or smoothen the concatenation \cite{chappell02}.

Even though there was a lot of noise in the output signal the transformed spectrum was audible. Naturally, with the presented transformation accuracy is was not possible to identify the synthesised sound signal as the target speaker.

% section Synthesis (end)

% chapter Discussion (end)