\chapter{Implementation} % (fold)
\label{cha:implementation}
The first step of implementation is to fit the source data to a Gaussian mixture model. Obviously the more data the better. However, the choice of number of mixtures is not trivial. When the source data is fitted to the GMM the posterior matrix $\mathbf{P}$ and the $\mathbf{D}$ matrix can be computed to a specific source speech segment $\mathbf{x}$ which we want to transform. The next step is the compute the unknown parameters $\boldsymbol{\mu}^y$ and $\mathbf{\Sigma}^{yx}$ with $\mathbf{P}$, $\mathbf{D}$ and the target speech segment ($\mathbf{y}$). The third step is to apply the conversion function \eqref{eq:conversion_function}.

Figure~\ref{fig:training_cf} shows the training procedure.
\begin{figure}[htbp]
  \centering
   \begin{tabular}[h]{c}
	\xymatrix{ 
\textbf{X}_{LSF}\ar[r] &*+<5mm>[F-,]{\txt{GMM}}\ar[r] &*+<5mm>[F-,]{\mathbf{P},\mathbf{D}} \ar[r] &*+<5mm>[F-,]{\mathbf{\Sigma}^{yx},\boldsymbol{\mu}^y} \\ % end line 1
\textbf{Y}_{LSF}\ar`[rrru][urrr]&&&
	}
  \end{tabular}
  \caption{Training Procedure of the Conversion Function}
  \label{fig:training_cf}
\end{figure}

\section{Alignment} % (fold)
\label{sec:alignment}
In the training of the $\boldsymbol{\mu}^y$ vector and the $\mathbf{\Sigma}^{yx}$ matrix, the source and target speech vectors need to be time-aligned. In this process the speech segments in split into 10 ms segments with additional $2.5$ ms overlap in both ends. The segments are analysed with LPC into a matrix of LP coefficients which are used to time align the speech signals with dynamic time warping, Section~\ref{sec:dynamic_time_warping}.

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=.8\textwidth]{fig/dtw.pdf}
		\caption{Distance Matrix used in Dynamic Time Warping}
		\label{fig:dtw}
	\end{center}
\end{figure}
Figure~\ref{fig:dtw} shows a distance matrix between two speech signals of \ca 4 seconds, where the horizontal axis represent the source signal and the vertical axis represent the target signal. The grey scale indicates the difference between two specific segments where the darker a square is the more similar the segments are. The red line depicts the shortest path, or the best match, between the two signals. The red lines also denotes which indices from the source vector we should use in time aligned version of the signal. Recursion \eqref{eq:dtw_recursion} is used with local constraints $\alpha=1, \beta=2$ and $\gamma=5$ and global constraints as shown in the rightmost plot of Figure~\ref{fig:dtw}.
% section Alignment (end)

\section{Training Data} % (fold)
\label{sec:training_data}
In the training of the Gaussian mixture model it is preferable to use as much training data as possible. 20 000 vectors of 10 ms where fitted to the GMM. This can be done once and for all. In the training process of the parameters $\mathbf{P}$, $\mathbf{D}$, $\mathbf{\Sigma}^{yx}$ and $\boldsymbol{\mu}^y$ the only data needed is the source data that are going to be converted and the target data.

The initialisation of the GMM affects the convergence rate and possibly the resulting estimate. However, when only diagonal covariance matrices are used, the initialisation does not have a significant effect \cite{reynolds93}. The GMM was initialised with vector quantisation (VQ) where the variance and priors was estimated from the quantised vectors.
% section Training data (end)

\section{Conversion Function Parameters} % (fold)
\label{sec:conversion_function_parameters}
The matrices in \eqref{eq:param_computed} could get really huge with a large amount of mixture models and long signal vectors. To simplify the computational complexity the covariance matrices can constrained to be diagonal \cite{stylianou98}. $\mathbf{\Sigma}^{yx}$ will become a $m\times p$ matrix and $\mathbf{D}$ will have dimensions $n\times m$. The new transfer function will then become
\begin{equation}
	\mathcal{F}(x_t^{(k)}) = \sum_{i=1}^{m}P(C_i \vert \mathbf{x}_t)[\mu_i^{y^{(k)}}+\sigma_i^{yx^{(k)}}  (x_t^{(k)}-\mu_i^{x^{(k)}})/\sigma_i^{xx^{(k)}}]
\end{equation}
where $k=1,\dots,p$ and denotes one LSF parameter.

The optimisation problem is divided into $p$ independent problems where the unknown vectors $\boldsymbol{\sigma}^{yx}$ and $\boldsymbol{\mu}^{y}$ have to calculate for each $k$. Both will have dimensions $m\times 1$.

The $\mathbf{D}^{(k)}$ matrix needs to be modified to fit the new scheme.
\begin{equation}
	\label{eq:D_matrix_new}
	\mathbf{D}^{(k)} = \begin{bmatrix}
		p_1(1)(x_1^{(k)} - \mu_1^{x^{(k)}})/\sigma_1^{xx^{(k)}} & \dots & p_1(m)(x_1^{(k)} - \mu_m^{x^{(k)}})/\sigma_m^{xx^{(k)}} \\
		p_2(1)(x_2^{(k)} - \mu_1^{x^{(k)}})/\sigma_1^{xx^{(k)}} & \dots & p_2(m)(x_2^{(k)} - \mu_m^{x^{(k)}})/\sigma_m^{xx^{(k)}} \\
		\vdots & & \vdots \\
		p_n(1)(x_n^{(k)} - \mu_1^{x^{(k)}})/\sigma_1^{xx^{(k)}} & \dots & p_n(m)(x_n^{(k)} - \mu_m^{x^{(k)}})/\sigma_m^{xx^{(k)}} \\
	\end{bmatrix}
\end{equation}
where $p_n(m)=P(C_m\vert \mathbf{x}_n)$.
% section Conversion Function (end)
% chapter Method (end)