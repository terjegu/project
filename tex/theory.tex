\chapter{Theory} % (fold)
\label{cha:theory}
The basic idea of the voice conversion is to make a LP analysis of the signal, extract the estimate from the real signal, adapt the coefficients to the new voice and reestimate the signal with the new coefficients as depicted in Figure~\ref{fig:VC}.
\begin{figure}[htbp]
  \centering
   \begin{tabular}[h]{c}
\xymatrix{ 
\mathbf{X} \ar[rr] & \ar[d]\ar[r] &*+<5mm>[F-,]{1-A(z)}\ar[rr]^{\mathbf{e}}& &*+<5mm>[F-,]{\frac{1}{1-\tilde{A}(z)}} \ar[r] & \mathbf{Y}\\ % end line 1
  &*+<5mm>[F-,]{\txt{LPC}} \ar[rr]^<(0.25){A(z)} & \ar[u]&*+<5mm>[F-,]{\txt{CF}}\ar `[ru]^<(0.5){\tilde{A}(z)} [ur]&  &}
  \end{tabular}
  \caption{Conversion System}
  \label{fig:VC}
\end{figure}

The system includes two parts, training and transformation. The challenge in this procedure is to find the conversion function explained in Section~\ref{sec:conversion_function}. In the training process of the conversion function, depicted in Figure~\ref{fig:VC_training}, the two input signals, source $x$ and target $y$, first need to be aligned in time, Section~\ref{sec:dynamic_time_warping}. The source data is classified into probabilistic Gaussian mixture models (GMM), Section~\ref{sec:gaussian_mixture_model}, and used in a class-specific transform \cite{stylianou09}.
\begin{figure}[htbp]
	\centering
	\begin{tabular}[h]{c}
		\xymatrix{ 
\mathbf{X} \ar[r] &*+<5mm>[F-,]{\txt{LPC}}\ar[d] &*+<5mm>[F-,]{\txt{GMM}}\ar `[rd]^{\boldsymbol{\Phi}} [dr] &\\ % end line 1
&*+<5mm>[F-,]{\txt{DTW}} \ar @<3pt>[rr]^<(0.15){\mathbf{X}} \ar @<-3pt>[rr]_<(0.15){\tilde{\mathbf{Y}}} & \ar[u] &*+<5mm>[F-,]{\txt{Train VC}}\\ % end line 2
\mathbf{Y} \ar[r] &*+<5mm>[F-,]{\txt{LPC}}\ar[u]&&}
	\end{tabular}
	\caption{Training of Voice Conversion Parameters}
	\label{fig:VC_training}
\end{figure}


\section{Conversion Function} % (fold)
\label{sec:conversion_function}
The available data for the conversion function is time-aligned source and target vectors, Section~\ref{sec:dynamic_time_warping}, which are of the same length $n$ and the pre-trained Gaussian mixture models, Section~\ref{sec:gaussian_mixture_model}. A conversion function suggested by Yannis Stylianou \etal \cite{stylianou95} is used in this paper:
\begin{defn}
	\label{defn:conversion_function}
	\begin{equation}
		\label{eq:conversion_function}
		\begin{split}
		\mathcal{F}(\mathbf{x}_t) =& E[\mathbf{x}_t\vert \mathbf{y}] = E[\mathbf{x}_t]E[\mathbf{y}\vert \mathbf{x}_t]\\
		 =& \sum_{i=1}^{m}P(C_i \vert \mathbf{x}_t)[\boldsymbol{\mu}_i^y + \mathbf{\Sigma}_i^{yx} \mathbf{\Sigma}_i^{xx^{-1}} (\mathbf{x}_t-\boldsymbol{\mu}_i^x)]
		\end{split}
	\end{equation}	
\end{defn}
The conversion function is a weighted sum of minimum mean square error (MMSE\abbrev{MMSE}{minimum mean square error}) estimate of the target vector $\mathbf{y}_t$ given the observed value of $\mathbf{x}_t$, which are jointly Gaussian \cite{stylianou98}. The MMSE is given by \cite{taletek}
\begin{equation}
	\label{eq:mmse}
	E[\mathbf{y}\vert \mathbf{x}=\mathbf{x}_t] = \boldsymbol{\mu}^y + \mathbf{\Sigma}^{yx} \mathbf{\Sigma}^{xx^{-1}} (\mathbf{x}_t-\boldsymbol{\mu}^x)
\end{equation}
where $E$ denotes the expectation, $\boldsymbol{\mu}$ is the mean
\begin{equation}
	\boldsymbol{\mu}^y = E[\mathbf{y}]
\end{equation}
and $\mathbf{\Sigma}$ is the cross-covariance matrix
\begin{equation}
	\mathbf{\Sigma}^{yx} = E[(\mathbf{y}-\boldsymbol{\mu}^y)(\mathbf{x}-\boldsymbol{\mu}^x)^T].
\end{equation}

If the gaussian components of the mixture could be separated, the conversion function $\mathcal{F}()$ is modifying the posterior mean and covariance of each component. Since this is not the case in practice the term is weighted by the posterior probability, or the conditional probability that the observed data $\mathbf{x}_t$ belongs to the component $C_i$.

The variables $\boldsymbol{\mu}^y$ and $\mathbf{\Sigma}^{yx}$ are unknown and need to be estimated such as the total squared conversion error is minimised
\begin{equation}
	\label{eq:conversion_error}
	\epsilon = \sum_{t} \norm{\mathbf{y}_t - \mathcal{F}(\mathbf{x}_t)}^2.
\end{equation}

Equation \eqref{eq:conversion_function} can be reformulated into a single matrix equation:
\begin{equation}
	\label{eq:least_square_problem}
	\begin{split}
		\mathbf{y} = &\mathbf{P}\boldsymbol{\mu^y} + \mathbf{D}\mathbf{\Sigma}^{yx} \\
		= & \begin{bmatrix}
			\mathbf{P}& \vdots &\mathbf{D}
		\end{bmatrix}
		\begin{bmatrix}
			\boldsymbol{\mu^y} \\
			\dots \\
			\mathbf{\Sigma}^{yx}
		\end{bmatrix}
	\end{split}
\end{equation}
where $\mathbf{y}$ is the target spectral vectors and $\mathbf{P}$ is a $n \times m$ matrix with the posterior probabilities
\begin{equation}
	\label{eq:P_matrix}
	\mathbf{P} = \begin{bmatrix}
		P(C_1\vert \mathbf{x}_1) & \dots & P(C_m\vert \mathbf{x}_1) \\
		P(C_1\vert \mathbf{x}_2) & \dots & P(C_m\vert \mathbf{x}_2) \\
		\vdots & & \vdots \\
		P(C_1\vert \mathbf{x}_n) & \dots & P(C_m\vert \mathbf{x}_n) \\
	\end{bmatrix}.
\end{equation}
$\mathbf{D}$ is a $n \times pm$ matrix defined as
\begin{equation}
	\label{eq:D_matrix}
	\mathbf{D} = \begin{bmatrix}
		P(C_1\vert \mathbf{x}_1)(\mathbf{x}_1 - \boldsymbol{\mu}_1^x)^T\mathbf{\Sigma}_1^{xx^{-1}} & \dots & P(C_m\vert \mathbf{x}_1)(\mathbf{x}_1 - \boldsymbol{\mu}_m^x)^T\mathbf{\Sigma}_m^{xx^{-1}} \\
		P(C_1\vert \mathbf{x}_2)(\mathbf{x}_2 - \boldsymbol{\mu}_1^x)^T\mathbf{\Sigma}_1^{xx^{-1}} & \dots & P(C_m\vert \mathbf{x}_2)(\mathbf{x}_2 - \boldsymbol{\mu}_m^x)^T\mathbf{\Sigma}_m^{xx^{-1}} \\
		\vdots & & \vdots \\
		P(C_1\vert \mathbf{x}_n)(\mathbf{x}_n - \boldsymbol{\mu}_1^x)^T\mathbf{\Sigma}_1^{xx^{-1}} & \dots & P(C_m\vert \mathbf{x}_n)(\mathbf{x}_n - \boldsymbol{\mu}_m^x)^T\mathbf{\Sigma}_m^{xx^{-1}} \\
	\end{bmatrix}.
\end{equation}

The two unknown matrices $\boldsymbol{\mu}^y$ and $\mathbf{\Sigma}^{yx}$ will have dimensions $m\times p$ and $m \times (p\times p)$ 
\begin{equation}
	\label{eq:v_matrix}
	\boldsymbol{\mu}^y = 
	\begin{bmatrix}
		\boldsymbol{\mu}^y_1 \vdots \boldsymbol{\mu}^y_2 \vdots \dots \vdots \boldsymbol{\mu}^y_m
	\end{bmatrix}^T
\end{equation}
\begin{equation}
	\label{gamma_matrix}
	\mathbf{\Sigma}^{yx} = 
	\begin{bmatrix}
		\mathbf{\Sigma}_1^{yx} \vdots \mathbf{\Sigma}_2^{yx} \vdots \dots \vdots \mathbf{\Sigma}_m^{yx}
	\end{bmatrix}^T.
\end{equation}

Equation \eqref{eq:least_square_problem} is a least-square problem which can be solved by the normal equations \cite{lawson74}
\begin{equation}
	\label{eq:param_computed}
	\begin{split}
		\left( 
		\begin{bmatrix}
			\mathbf{P}^T \\
			\dots \\
			\mathbf{D}^T
		\end{bmatrix}
		\begin{bmatrix}
			\mathbf{P} & \vdots & \mathbf{D}
		\end{bmatrix}
		 \right)
		\begin{bmatrix}
			\boldsymbol{\mu}^y \\
			\dots \\
			\mathbf{\Sigma}^{yx}
		\end{bmatrix}
		= &
		\begin{bmatrix}
			\mathbf{P}^T \\
			\dots \\
			\mathbf{D}^T
		\end{bmatrix}
		\mathbf{y} \\ % end line 1
		\begin{bmatrix}
			\boldsymbol{\mu}^y \\
			\dots \\
			\mathbf{\Sigma}^{yx}
		\end{bmatrix}
		= &
		\left( 
		\begin{bmatrix}
			\mathbf{P}^T \\
			\dots \\
			\mathbf{D}^T
		\end{bmatrix}
		\begin{bmatrix}
			\mathbf{P} & \vdots & \mathbf{D}
		\end{bmatrix}
		 \right)^{-1}
		\begin{bmatrix}
			\mathbf{P}^T \\
			\dots \\
			\mathbf{D}^T
		\end{bmatrix}
		\mathbf{y} \\ % end line 2
	\end{split}
\end{equation}
% section Conversion Function (end)

\section{Dynamic Time Warping} % (fold)
\label{sec:dynamic_time_warping}
Feature comparison of two signals gives more sense if they are aligned in time. If the signals are not aligned they will not appear as equal if they are in fact equal. Dynamic time warping (DTW\abbrev{DTW}{dynamic time warping}) is an dynamic programming concept to warp two sequence such as the difference in the signals are minimised. The signals are segmented into small parts which are compared and given a similarity-score. There are many ways to assign this score, \eg the \emph{Itakura distance} \cite{itakura90},
\begin{equation}
	\label{eq:dtw_distance}
		d(\mathbf{a},\mathbf{b}) = \log\frac{\mathbf{a}^T \mathbf{R}_s \mathbf{a}}{\mathbf{b}^T \mathbf{R}_s \mathbf{b}}
\end{equation}
where $\mathbf{a}$ is a column vector of LP coefficients of one segment from the source signal, $\mathbf{b}$ is a column vector of LP coefficients of one segments from the target vector and $\mathbf{R}_s$ is the autocorrelation matrix of the source signal. The numerator of \eqref{eq:dtw_distance} is the residual, or the energy of the error, from the LP of the source signal with the source coefficients $\mathbf{a}$ whilst the denominator is the residual from the LP of the \emph{source} signal with the \emph{target} coefficients $\mathbf{b}$. 

The minimum distance between the two signals can be found by solving the recursion \cite{taletek}
\begin{equation}
	\label{eq:dtw_recursion}
	\begin{split}
		D(i,j) = \min \bigl[&D(i-1,j-1)+\alpha d(\mathbf{A}_i,\mathbf{B}_j),\\
		& D(i-1,j)+\beta d(\mathbf{A}_i,\mathbf{B}_j),\\
		& D(i,j-1)+\beta d(\mathbf{A}_i,\mathbf{B}_j),\\
		& D(i-2,j-1)+\gamma d(\mathbf{A}_i,\mathbf{B}_j),\\
		& D(i-1,j-2)+\gamma d(\mathbf{A}_i,\mathbf{B}_j)\bigr]		
	\end{split}
\end{equation}
where $\mathbf{A}_i$ denotes the LP coefficients of segment $i$. 
% I.e. the shortest path from $A$ to $C$ is the shortest path from $A$ to $B$ plus the shortest path from $B$ to $C$. 
The problem is broken down into simpler subproblems, hence dynamic programming. 

The possible paths are shown in Figure~\ref{fig:dtw_shortest_path}. The weights $\alpha,\beta$ and $\gamma$, called local constraints, are optional and could have a significant effect on the outcome. For instance, the $\beta$ weights can be assigned a larger weight to avoid multiple skipping or repetition of frames. Global constraints can be applied in addition to the local constraints which limits the resulting path by a borders to guarantee a maximum modification of the signals.

\begin{figure}[htbp]
	\begin{center}
		\setlength{\unitlength}{0.8cm}
		\begin{picture}(8,8)(0,0)
		\put(0,0){\vector(1,0){8}}
		\put(7,-0.5){$\mathbf{A}$}
		\put(-0.5,7){$\mathbf{B}$}
		\put(0,0){\vector(0,1){8}}
		\put(7,7){\circle*{0.3}}
		\put(4,4){\vector(1,1){2.8}}
		\put(4,7){\vector(1,0){2.8}}
		\put(7,4){\vector(0,1){2.8}}
		\put(1,4){\vector(2,1){5.8}}
		\put(4,1){\vector(1,2){2.9}}
		\put(5,4){$\gamma$}
		\put(3.7,5.7){$\gamma$}
		\put(7.1,5){$\beta$}
		\put(5,7.1){$\beta$}
		\put(5,5.4){$\alpha$}
		\put(2.6,7){\tiny{$(i-1,j)$}}
		\put(6.6,3.7){\tiny{$(i,j-1)$}}
		\put(0.1,3.7){\tiny{$(i-2,j-1)$}}
		\put(3.1,0.7){\tiny{$(i-1,j-2)$}}
		\put(3,3.7){\tiny{$(i-1,j-1)$}}
		\end{picture}
		\caption{DTW shortest path calculations}
		\label{fig:dtw_shortest_path}
	\end{center}
\end{figure}

% section Dynamic Time Warping (end)

\section{Gaussian Mixture Model} % (fold)
\label{sec:gaussian_mixture_model}
Because of the wide variety of speech sound, a single global transform would require enormous complexity of both training data and the transformation function. A probabilistic Gaussian mixture models is a simpler and faster approach. The GMM is a classical parametric model used in many pattern recognition techniques \cite{stylianou98}. The GMM assumes that the probability distribution of the observed parameters takes the following parametric form
\begin{equation}
	\label{eq:gmm}
	p(\mathbf{x}) = \sum_{i=1}^{m} \alpha_i N(\mathbf{x}; \boldsymbol{\mu}_i, \mathbf{\Sigma}_i)
\end{equation}
where $m$ is the number of mixture models and $N(\mathbf{x}; \boldsymbol{\mu}_i, \mathbf{\Sigma}_i)$ denotes the p-dimensional normal distribution with the mean vector $\boldsymbol{\mu}_i$ and covariance matrix $\mathbf{\Sigma}_i$ defined by
\begin{equation}
	N(\mathbf{x}; \boldsymbol{\mu}, \mathbf{\Sigma}) = \frac{1}{\sqrt{(2\pi)^p\abs{\mathbf{\Sigma}}}} \exp\left[ -\frac{1}{2} (\mathbf{x} -\boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} -\boldsymbol{\mu})\right]
\end{equation}
and $\alpha_i$ in \eqref{eq:gmm} is the prior probability of class $i$ with constraints $\sum_{i=1}^{M}\alpha_i = 1$ and $\alpha_i \geq 0$. The input vectors, $\mathbf{x_i}$, are assumed to be independent \cite{stylianou98}.

The conditional probability that a given observation vector $\mathbf{x}$ belongs to the component $C_i$ of the GMM is given by Bayes' rule \cite{statistikk} as
\begin{equation}
	\label{eq:bayes}
	P(C_i\vert \mathbf{x}) = \frac{\alpha_i N(\mathbf{x}; \boldsymbol{\mu}_i, \mathbf{\Sigma}_i)}{\sum_{j=1}^{m}\alpha_j N(\mathbf{x}; \boldsymbol{\mu}_j, \mathbf{\Sigma}_j)}.
\end{equation}

The parameters $\boldsymbol{\alpha}, \boldsymbol{\mu}$ and $ \mathbf{\Sigma}$ can be estimated with the expectation maximisation algorithm, Section~\ref{sec:expectation_maximisation}.

% section Gaussian Mixture Model (end)

\section{Expectation Maximisation} % (fold)
\label{sec:expectation_maximisation}
The expectation maximisation, EM \abbrev{EM}{expectation maximisation}, algorithm is an iterative algorithm for unsupervised learning in which component information is unavailable or only partly available \cite{taletek}. It estimates the model parameters by maximising the log-likelihood of incomplete data and maximising the expectation of log-likelihood from complete data.

We need to determine the parameter $\mathbf{\Phi} = \{\boldsymbol{\alpha}, \boldsymbol{\mu}, \mathbf{\Sigma}\}$ which maximises $P(Y=y\vert \mathbf{\Phi})$ where $y$ is the observed training data. We assume some parameter vector $\mathbf{\Phi}$ end estimate the probability of some unknown data $x$ occurred in the generation of $y$. We then compute a new $\bar{\mathbf{\Phi}}$ which is the maximum likelihood of $\mathbf{\Phi}$ and set the new $\bar{\mathbf{\Phi}}$ to be $\mathbf{\Phi}$ and repeat the process iteratively until the process converges \cite{taletek}. The process will converge if we choose $\bar{\mathbf{\Phi}}$ such that 
\begin{equation}
	\label{eq:q_criteria}
	Q(\mathbf{\Phi},\bar{\mathbf{\Phi}})\geq Q(\mathbf{\Phi},\mathbf{\Phi})
\end{equation}
where 
\begin{equation}
	\label{eq:q_function}
	Q(\mathbf{\Phi},\bar{\mathbf{\Phi}}) = E[\log P(X,Y=y\vert \bar{\mathbf{\Phi}})].
\end{equation}
% section Estimation Maximasation (end)

\section{Signal Representation} % (fold)
\label{sec:signal_representation}
The main signal representation of choice is linear predictive coding (LPC\abbrev{LPC}{linear predictive coding}) also known as auto-regressive (AR\abbrev{AR}{auto-regressive}) modelling. 

While the LPC representation is a delicate representations of speech signals, it is not the best choice when it comes to manipulating the parameters in the voice conversion because they are not necessarily stable. A number of equivalent representations can be used instead for this purpose such as line spectrum frequencies (LSF\abbrev{LSF}{line spectrum frequencies}) \cite{taletek}.

\subsection{Linear Predictive Coding} % (fold)
\label{sub:lpc}
LPC approximates the signal as an all-pole filter with a sufficient number of poles, \eg $p=16$, by predicting the current sample as a linear combination of its last $p$ samples \cite{digsig}
\begin{equation}
	\tilde{x}[n] = \sum_{k=1}^{p}a_k x[n-k].
\end{equation}
The prediction error by this representation is 
\begin{equation}
	\begin{split}
		e[n]= & x[n]-\tilde{x}[n]\\
		= & x[n]-\sum_{k=1}^{p}a_k x[n-k].
	\end{split}
\end{equation}

The predictor coefficients, $a_k$, can be estimated by the minimum mean square technique which chooses the coefficients that minimise the total prediction error $E$.
\begin{equation}
	\label{eq:prediction_error}
	\begin{split}
		E = & \sum_{n}e^2[n]\\
		= & \sum_{n}\left( x[n]-\sum_{k=1}^{p}a_k x[n-k] \right)^2
	\end{split}
\end{equation}
This can be obtained by taking the derivative of \eqref{eq:prediction_error} with respect to $a_i$ and equating to $0$. This yields a set of $p$ linear equations with $p$ unknown parameters \cite{digsig} which easily can be solved.
% subsection Linear Predictive Coding (end)

\subsection{Line Spectral Frequencies} % (fold)
\label{sub:line_spectral_frequencies}
LSF is an equivalent representation of LPC witch can be directly derived from the LP coefficients $A(z)$ and back again. It is derived from the roots of the polynomials $P(z)$ and $Q(z)$ 
\begin{equation}
	\label{eq:p_z}
	P(z) = A(z)+z^{-(p+1)}A(z^{-1})
\end{equation}
\begin{equation}
	\label{eq:q_z}
	Q(z) = A(z)-z^{-(p+1)}A(z^{-1}).
\end{equation}
The derivation of the first LSF coefficients are shown in \cite[p. 304]{taletek}.

In the voice conversion we can therefor transform the LP coefficients to LSF coefficients before the adaption and back agin before the re-estimation of the signal depicted in Figure~\ref{fig:lpc_to_lsf}.
\begin{figure}[htbp]
	\centering
	\begin{tabular}[h]{c}
		\xymatrix{ 
		A(z) \ar[r] &*+<5mm>[F-,]{ LPC\rightarrow LSF} \ar[r] &*+<5mm>[F-,]{	VC}\ar[r] &*+<5mm>[F-,]{LSF\rightarrow LPC} \ar[r] & \hat{A}(z)}    
	\end{tabular}
	\caption{LPC to LSF Transformation in Voice Conversion}
	\label{fig:lpc_to_lsf}
\end{figure}
% subsection Line Spectral Frequencies (end)
% section Signal Representation (end)

% \subsection{Mel-Frequency Cepstrum} % (fold)
% \label{sub:mel_frequency_cepstrum}
% In the dynamic time warping algorithm presented in Section~\ref{sec:dynamic_time_warping} MFC coefficients are used instead of LP coefficients because they have the feature of Euclidean measures.
% 
% The cepstrum of a signal is a homomorphic transformation to the \emph{quefrency} domain \cite{taletek}. The Mel-frequency cepstrum is a real cepstrum with a nonlinear frequency scale which approximates the behaviour of the human auditory system. The process of computing the MFC coefficents are depicted in Figure~\ref{fig:mfcc}.
% 
% The Mel-scale is defined as \cite{taletek}
% \begin{equation}
% 	B(f) = 1125\ln(1+f/700).
% \end{equation}
%  \begin{figure}[htbp]
%   \centering
%   \begin{tabular}[h]{c}
% \xymatrix{ 
%   x[n] \ar[r] &*+<5mm>[F]{\text{DFT}}\ar[r] &*+<5mm>[F]{\abs{\cdot}^2}\ar[r] &*+<5mm>[F]{\text{MEL}} \ar[r] &*+<5mm>[F]{\log(\cdot)} \ar[r] &*+<5mm>[F]{\text{DCT}} \ar[r] & c[n]}    
%   \end{tabular}
%   \caption{Computation of Mel-frequency cepstrum coefficients}
%   \label{fig:mfcc}
% \end{figure}
% 
% % subsection Mel-Frequency Cepstrum (end)
% chapter Theory (end)