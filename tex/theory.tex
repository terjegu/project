\chapter{Theory} % (fold)
\label{cha:theory}

\begin{figure}[htbp]
  \centering
   \begin{tabular}[h]{c}
\xymatrix{ 
x[n] \ar[r] &*+<5mm>[F]{\text{Buffer}} \ar[d]\ar[r] &*+<5mm>[F]{1-A(z)}\ar[rr]& &*+<5mm>[F]{\frac{1}{1-A(z)}} \ar[r] & y(x)\\
  &*+<5mm>[F]{LP} \ar[r]^{A(z)}& \ar[u]\ar[r]&*+<5mm>[F]{\text{VC}}\ar[r]^{\tilde{A}(z)}& \ar[u]&}
  \end{tabular}
  \caption{Voice Conversion}
  \label{fig:ADPCM}
\end{figure}

\section{Gaussian Mixture Model} % (fold)
\label{sec:gaussian_mixture_model}
The GMM is a classical parametric model used in many pattern recognition techniques \cite{stylianou98}. The GMM assumes that the probability distribution of the observed parameters takes the following parametric form
\begin{equation}
	\label{eq:gmm}
	p(\mathbf{x}) = \sum_{i=1}^{m} \alpha_i N(\mathbf{x}; \boldsymbol{\mu}_i, \mathbf{\Sigma}_i)
\end{equation}
where $N(\mathbf{x}; \mathbf{\mu}_i, \mathbf{\Sigma}_i)$ denotes the p-dimensional normal distribution with the mean vector $\boldsymbol{\mu}$ and covariance matrix $\mathbf{\Sigma}$ defined by
\begin{equation}
	N(\mathbf{x}; \boldsymbol{\mu}_i, \mathbf{\Sigma}_i) = \frac{1}{\sqrt{(2\pi)^p\abs{\mathbf{\Sigma}}}} \exp\left[ -\frac{1}{2} (\mathbf{x} -\boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} -\boldsymbol{\mu})\right]
\end{equation}
and the $\alpha_i$ in \eqref{eq:gmm} is a normalised positive scalar; $\sum_{i=1}^{M}\alpha_i = 1$ and $\alpha_i \geq 0$. The $\mathbf{x_i}$ vectors are assumed to be independent.

In the GMM, each class is described by its center, $\boldsymbol{\mu}_i$, and the spreading around the center of the class, $\mathbf{\Sigma_i}$. The frequency of each class in the observation is represented by mixture weights, $\alpha_i$ \cite{stylianou98}. The conditional probability that a given observation vector $\mathbf{x}$ belongs to the class $C_i$ of the GMM is given by Bayes' rule \cite{statistikk} as
\begin{equation}
	\label{eq:bayes}
	P(C_i\vert \mathbf{x}) = \frac{\alpha_i N(\mathbf{x}; \boldsymbol{\mu}_i, \mathbf{\Sigma}_i)}{\sum_{j=1}^{m}\alpha_j N(\mathbf{x}; \boldsymbol{\mu}_j, \mathbf{\Sigma}_j)}.
\end{equation}
% section Gaussian Mixture Model (end)

\section{Expectation Maximisation} % (fold)
\label{sec:expectation_maximisation}
The Expectation maximisation, EM \abbrev{EM}{Expectation Maximisation}, algorithm is an iterative algorithm for unsupervised learning in which class information is unavailable or only partly available \cite{taletek}.
% section Estimation Maximasation (end)

\section{Time-scale modification} % (fold)
\label{sec:time_scale_modification}

% section Time-scale modification (end)

\section{Pitch-scale modification} % (fold)
\label{sec:pitch_scale_modification}

% section Pitch-scale modification (end)

\section{Spectral modification} % (fold)
\label{sec:spectral_modification}

\subsection{Rule-based} % (fold)
\label{sub:rule_based}

% subsection Rule-based (end)

\subsection{Statistical} % (fold)
\label{sub:statistical}

% subsection Statistical (end)

\subsection{Conversion function} % (fold)
\label{sub:conversion_function}
Conversion function \cite{stylianou95}
\begin{equation}
	\label{eq:conversion_function}
	F(\mathbf{x}_t) = \sum_{i=1}^{m}P(C_i \vert \mathbf{x}_t)[v_i + \boldsymbol{\Gamma}_i \mathbf{\Sigma}_i^{-1}(\mathbf{x}_t)-\mu_i]
\end{equation}

Squared conversion error should be minimised.
\begin{equation}
	\epsilon=\sum_{t=1}^{n}\norm{\mathbf{y}_t - F(\mathbf{y}_t)}^2
\end{equation}
% subsection Conversion function (end)
% section Spectral modification (end)
% chapter Theory (end)